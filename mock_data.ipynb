{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alex Malz (NYU), Gautham Narayan (STSci), Renee Hlozek (U. Toronto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for making mock data for PLAsTiCC, in order to test the metrics.  I'm going to model it off the [variability tree](https://obswww.unige.ch/~mowlavi/Images/variability_tree.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "import numpy.random as npr\n",
    "import bisect\n",
    "import scipy.stats as sps\n",
    "import sklearn as skl\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider four classes: `A`, `B`, `C`, and `D`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = ['A', 'B', 'C', 'D']\n",
    "# n_classes = len(classes)\n",
    "n_classes = 10\n",
    "class_names = [''.join(random.sample(string.ascii_lowercase, 2)) for i in range(n_classes)]\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assign them probabilites that are random draws from a half-Cauchy distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0., 1., 100)\n",
    "plt.plot(x, sps.halfcauchy.pdf(x))\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$p(x)$')\n",
    "plt.savefig('halfcauchy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_probs = np.array([1., 2., 3., 4.])\n",
    "class_probs = sps.halfcauchy.rvs(size=n_classes) + (1. + np.arange(n_classes)[::-1]) / n_classes\n",
    "class_probs /= np.sum(class_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw true classes from this non-uniform discrete distribution and express them as a binary matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obj = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = np.cumsum(class_probs)\n",
    "truth = np.zeros((n_obj, n_classes))\n",
    "for t in truth:\n",
    "    r = np.random.uniform()\n",
    "    t[bisect.bisect(cdf, r)] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the drawn classes match the underlying probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((str('drawn class probabilities: '), np.mean(truth, axis=0)))\n",
    "print((str('true class probabilities: '), class_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best classifier is perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(n_obj, n_classes))\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('classification probabilites')\n",
    "ax1.matshow(sub1.T, vmin=0., vmax=1.)\n",
    "ax1.set_yticks(np.arange(n_classes))\n",
    "ax1.set_yticklabels(class_names)\n",
    "ax1.set_xticklabels(range(n_obj))\n",
    "ax1.set_xlabel('objects')\n",
    "ax1.set_ylabel('classes')\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.set_title('difference from truth')\n",
    "ax2.matshow(sub1.T - truth.T, vmin=-1., vmax=1.)\n",
    "ax2.set_yticks(np.arange(n_classes))\n",
    "ax2.set_yticklabels(class_names)\n",
    "ax2.set_xticklabels(range(n_obj))\n",
    "ax2.set_xlabel('objects')\n",
    "ax2.set_ylabel('classes')\n",
    "\n",
    "fig.savefig('perfect.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example: classifier that's right half the time and wrong evenly (no covariance between classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = 0.5 * sub1 + np.ones((n_obj, n_classes)) / n_classes\n",
    "sub2 /= np.sum(sub2, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(n_obj, n_classes))\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('classification probabilites')\n",
    "ax1.matshow(sub2.T, vmin=0., vmax=1.)\n",
    "ax1.set_yticks(np.arange(n_classes))\n",
    "ax1.set_yticklabels(class_names)\n",
    "ax1.set_xticklabels(range(n_obj))\n",
    "ax1.set_xlabel('objects')\n",
    "ax1.set_ylabel('classes')\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.set_title('difference from truth')\n",
    "ax2.matshow(sub2.T - truth.T, vmin=-1., vmax=1.)\n",
    "ax2.set_yticks(np.arange(n_classes))\n",
    "ax2.set_yticklabels(class_names)\n",
    "ax2.set_xticklabels(range(n_obj))\n",
    "ax2.set_xlabel('objects')\n",
    "ax2.set_ylabel('classes')\n",
    "\n",
    "fig.savefig('noiseless_random.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "too uniform! add some jitter to the tune of 90% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3 = sub2 + 0.1 * sps.halfcauchy.rvs(size=(n_obj, n_classes))\n",
    "sub3 /= np.sum(sub3, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(n_obj, n_classes))\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('classification probabilites')\n",
    "ax1.matshow(sub3.T, vmin=0., vmax=1.)\n",
    "ax1.set_yticks(np.arange(n_classes))\n",
    "ax1.set_yticklabels(class_names)\n",
    "ax1.set_xticklabels(range(n_obj))\n",
    "ax1.set_xlabel('objects')\n",
    "ax1.set_ylabel('classes')\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.set_title('difference from truth')\n",
    "ax2.matshow(sub3.T - truth.T, vmin=-1., vmax=1.)\n",
    "ax2.set_yticks(np.arange(n_classes))\n",
    "ax2.set_yticklabels(class_names)\n",
    "ax2.set_xticklabels(range(n_obj))\n",
    "ax2.set_xlabel('objects')\n",
    "ax2.set_ylabel('classes')\n",
    "\n",
    "fig.savefig('noisy_random.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(sub3 - truth).flatten())\n",
    "plt.savefig('check_noise.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub3[2])\n",
    "print(truth[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub3[4])\n",
    "print(truth[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want a covariance between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = np.eye(n_classes) + 0.1 * sps.halfcauchy.rvs(size=(n_classes, n_classes))\n",
    "confmat[0][1] += 1.\n",
    "confmat[1][0] += 1.\n",
    "confmat /= np.sum(confmat, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(confmat, vmin=0., vmax=1.)\n",
    "plt.savefig('small_confmat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub4 = np.empty((n_obj, n_classes))\n",
    "for t in range(n_obj):\n",
    "    mask = np.where(truth[t] == 1.)\n",
    "#     print(mask)#, confmat[mask], sub3[t])\n",
    "    sub4[t,:] = confmat[mask] * sub3[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(n_obj, n_classes))\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('classification probabilites')\n",
    "ax1.matshow(sub4.T, vmin=0., vmax=1.)\n",
    "ax1.set_yticks(np.arange(n_classes))\n",
    "ax1.set_yticklabels(class_names)\n",
    "ax1.set_xticklabels(range(n_obj))\n",
    "ax1.set_xlabel('objects')\n",
    "ax1.set_ylabel('classes')\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.set_title('difference from truth')\n",
    "ax2.matshow(sub4.T - truth.T, vmin=-1., vmax=1.)\n",
    "ax2.set_yticks(np.arange(n_classes))\n",
    "ax2.set_yticklabels(class_names)\n",
    "ax2.set_xticklabels(range(n_obj))\n",
    "ax2.set_xlabel('objects')\n",
    "ax2.set_ylabel('classes')\n",
    "\n",
    "fig.savefig('confmat_based.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try a metric!  _Note that the confusion matrix, however, reduces probabilities to point estimates!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred4 = np.argmax(sub4, axis=1)\n",
    "classids = np.arange(n_classes)\n",
    "# confusion_matrix(classids[np_argmax(truth)], classids[pred4)\n",
    "                                                      \n",
    "y_truth = classids[np.argmax(truth, axis=1)]\n",
    "y_pred = classids[np.argmax(sub4, axis=1)]\n",
    "cnf_matrix = confusion_matrix(y_truth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "# cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classids,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.savefig('big_confmat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_loss(truth, sub1))#perfect\n",
    "print(log_loss(truth, sub2))#guesses\n",
    "print(log_loss(truth, sub3))#noisy\n",
    "print(log_loss(truth, sub4))#correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "every deterministic classifier corresponds to a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what about an actual probabilistic classifier?\n",
    "\n",
    "* knn: make distances and create tree (sklearn.KDTree)\n",
    "* rf: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's permit hierarchical classes, so `C` and `D` may be subclasses of `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (not)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
